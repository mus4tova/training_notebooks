# training_projects

These small notebooks were created at the beginning of my professional journey to familiarize myself with classical machine learning algorithms, strengthen my skills, gain practical experience, and better understand the essence and principles of core ML algorithms. Later, as I progressed professionally, I made some improvements to my initial projects, enhanced the metrics, and added deeper data analysis.

1. Click Prediction: Binary classification task; exploratory data analysis (EDA); models: logistic regression, neural network.
2. Credit Score: Multiclass classification task; EDA; models: KNeighborsClassifier, Naive Bayes, logistic regression, random forest with hyperparameter tuning.
3. Customer Segmentation: Clustering; EDA; models: k-means.
4. Diabetes Prediction: Binary classification task; EDA; models: KNeighborsClassifier, Naive Bayes, CatBoost, random forest, support vector machine.
5. Fake News Prediction: NLP, binary classification; stemming; vectorizing; models: logistic regression.
6. House Price Prediction: Regression task; EDA; models: random forest, XGBoost, linear regression, LGBM regressor, neural network, CatBoost, ridge regression, BayesianRidge, SVM.
7. IMDb Review Classification: NLP, binary classification task; vectorizing; models: neural network with and without embedding layers.
8. Insurance Cost Prediction: Regression task; EDA; models: random forest, XGBoost, linear regression, LGBM regressor, neural network, CatBoost, ridge regression, BayesianRidge, SVM.
9. Iris Species: Multiclass classification task (3 classes); models: KNeighborsClassifier, Naive Bayes, logistic regression, random forest, LinearSVC.
10. MNIST Recognition: Computer vision (CV), multiclass classification (10 classes); models: neural network.
11. Spam Mail Prediction: NLP, binary classification task; vectorizing; models: logistic regression.
12. Titanic: Binary classification task; EDA; models: Keras neural network, random forest, and logistic regression with hyperparameter tuning.





